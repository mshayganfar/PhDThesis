\documentclass[12pt]{report}

\usepackage[titletoc]{appendix}
\usepackage{datetime}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage[explicit]{titlesec}
\usepackage{tocloft}
\usepackage{amssymb}
\usepackage{longtable}
\usepackage{multirow}
\usepackage[english]{babel}

\usepackage{subcaption}
\usepackage{subfig}

\usepackage{url}

\usepackage{rotating}

\usepackage{varwidth}
\usepackage{setspace}
\usepackage{perpage}
\usepackage{enumerate}
\MakePerPage{footnote}

\usepackage{tikz}
\usetikzlibrary{arrows,positioning,automata}
\usepackage[noend]{algpseudocode}

\usepackage{mathtools}

\usepackage{caption}

\renewcommand{\familydefault}{\rmdefault}
\renewcommand\cftchapaftersnum{.}
\renewcommand\cftchapdotsep{\cftdotsep}

\setlength{\textheight}{8.63in}
\setlength{\textwidth}{5.9in}
\setlength{\topmargin}{-0.2in}
\setlength{\oddsidemargin}{0.3in}
\setlength{\evensidemargin}{0.3in}
\setlength{\headsep}{0.0in}

\titleformat{\chapter}[block]
  {\Large\filcenter\bfseries}{\MakeUppercase{Chapter \thechapter}\\}{0em}{\MakeUppercase{#1}}
  
\titleformat{\section}[block]
  {\large\bfseries}{\thesection}{0.5em}{#1}
  
\titleformat{\subsection}[block]
  {\normalsize\bfseries}{\thesubsection}{0.5em}{#1}
  

\newcommand{\signature}{\rule{3in}{1.2pt}}
\newcommand{\thesistitle}{Affective Motivational Collaboration Theory}
\newdateformat{monthyear}{\monthname[\THEMONTH] \THEYEAR}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\linespread{1.5}

\begin{document}

\begin{titlepage}
\begin{center}
\large\textbf{\thesistitle}\\[0.5em]

\large\textnormal{by}\\
\large\textnormal{Mahni Shayganfar - mshayganfar@wpi.edu}\\[0.5em]

\large\textnormal{A PhD Dissertation}\\[0.5em]

\large\textnormal{Presented at}\\[0.5em]
\large\textsc{WORCESTER POLYTECHNIC INSTITUTE}\\[0.5em]
\large\textnormal{in partial fulfillment of the requirements for the}\\[0.5em]
\large\textnormal{DOCTOR OF PHILOSOPHY}\\[0.5em]
\large\textnormal{in}\\
\large\textnormal{Computer Science}\\
\large\textnormal{November 2016}\\[0.75em]
\end{center}

\noindent\large\textsc{Approved}\\[0.5em]
\Large\textnormal{\signature}\\
\normalsize\textnormal{Professor Charles Rich, Thesis Advisor}\\[0.5em]
\Large\textnormal{\signature}\\
\normalsize\textnormal{Professor Candace L. Sidner, Thesis Co-Advisor}\\[0.5em]
\Large\textnormal{\signature}\\
\normalsize\textnormal{Professor John E. Laird, Thesis Committee
Member}\\[0.5em] \Large\textnormal{\signature}\\
\normalsize\textnormal{Professor Stacy Marsella, Thesis Committee Member}
\end{titlepage}

\thispagestyle{empty}
\vspace*{\fill}
  \begin{center}
    \textcopyright \hspace{0.5em} Copyright by Mahni Shayganfar 2016

    All Rights Reserved
  \end{center}
\vspace*{\fill}
\newpage

\pagenumbering{roman}

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Abstract Here!

\pagebreak

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

Acknowledgments Here!

\pagebreak

\tableofcontents
\pagebreak

\listoffigures
\pagebreak

\listoftables
\pagebreak

%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}
%\pagebreak

\pagenumbering{arabic}

\chapter{Introduction}
\label{ch:introduction}

\section{Motivation}

The idea of robots or other intelligent agents living in a human environment has
been a persistent dream from science fiction books to artificial intelligence
and robotic laboratories. Collaborative robots are expected to become an
integral part of humans' environment to accomplish their industrial and
household tasks. In these environments, humans will be involved in robots'
operations and decision-making processes. The involvement of humans influences
the efficiency of robots' interaction and performance, and makes the robots
sensitive to humans' cognitive abilities and behaviors.

A key aspect of the sociability of robots is their ability to collaborate with
humans in the same environment. Collaboration is a coordinated activity in which
the participants work jointly to satisfy a shared goal
\cite{grosz:plans-discourse}. There are many challenges in achieving a
successful collaboration between robots and humans. To meet these challenges, it
is crucial to understand what makes a collaboration not only successful, but
also efficient. Existing computational models of collaboration explain some of
the important concepts underlying collaboration; such as the presence of a
reason for collaborators' commitment, and the necessity of communicating about
mental states in order to maintain progress over the course of a collaboration.
The most prominent collaboration theories are based on plans and intentions
\cite{cohen:teamwork} \cite{grosz:plans-discourse}
\cite{Litman:discourse-commonsense}, and are derived from Bratman's BDI
architecture \cite{bratman:intentions-plans}. Two theories, Joint Intentions
\cite{cohen:teamwork} and SharedPlans
\cite{grosz:planning-acting,grosz:collaboration,grosz:plans-discourse}, have
been used to support teamwork and collaboration between humans and robots or
virtual agents \cite{breazeal:humanoid-robots}
\cite{montreuil:planning-robot-activity} \cite{sidner:enagagement-robot}
\cite{yen:cast}. However, these theories explain only the structure of a
collaboration. For instance, in SharedPlans theory collaborators build a shared
plan containing a collection of beliefs and intentions about the actions in the
plan. Collaborators communicate these beliefs and intentions via utterances
about actions that contribute to the shared plan. This communication leads to
the incremental construction of a shared plan, and ultimately successful
completion of the collaboration. In contrast, in Joint Intentions theory, the
notion of joint intention is viewed as a persistent commitment of the team
members to a shared goal. In this theory, once an agent enters into a joint
commitment with other agents, it should communicate its private beliefs to other
team members.

Although existing collaboration theories explain the important elements of a
collaboration structure, the underlying processes required to dynamically
create, use, and maintain the elements of this structure are largely
unexplained. For instance, a general mechanism has yet to be developed that
allows an agent to effectively integrate the influence of its collaborator's
perceived or anticipated emotions into its own cognitive mechanisms to prevent
shared task failures while maintaining collaborative behavior. Therefore, a
process view of collaboration must include certain key elements. It should
inherently involve social interactions since all collaborations occur between
social agents, and it should essentially constitute a means of modifying the
content of social interaction as the collaboration unfolds. The underlying
processes of emotions possess these two properties, and social functions of
emotions explain some aspects of the underlying processes in collaboration. This
thesis makes the case for emotion-driven processes within collaboration and
demonstrates how it furthers collaboration between humans and robots.

\section{Thesis Statement and Scope}

In this thesis, we develop and validate a framework based on \textit{Affective
Motivational Collaboration Theory} which can improve the effectiveness of
collaboration between agents/robots and humans. This thesis is established based
on the reciprocal influence of collaboration structure and the appraisal
processes in a dyadic collaboration. We focus only on two-participant
collaboration; teamwork collaboration is out of our scope. Furthermore, this
work focuses on a) the influence of emotion-regulated processes on the
collaboration structure, and b) prediction of the observable behaviors of the
other during a collaborative interaction.

We describe the cognitive processes involved in a collaboration in the context
of a cognitive architecture. There are several well-developed cognitive
architectures, e.g., Soar \cite{laird:soar} and ACT-R \cite{anderson:act-r},
each with different approaches to defining the basic cognitive and perceptual
operations. There have also been efforts to integrate affect into these
architectures \cite{dancy:actR-physiology-affect, marinier:behavior-emotion}. In
general, however, these cognitive architectures do not focus on processes to
specifically produce emotion-regulated goal-driven collaborative behaviors. At
the same time, existing collaboration theories, e.g., SharedPlans
\cite{grosz:plans-discourse} theory, focus on describing the structure of a
collaboration in terms of fundamental mental states, e.g., mutual beliefs or
joint intentions. However, they do not describe the associated processes, their
relationships, and influences on each other. \textit{Affective Motivational
Collaboration Theory} deals with some of the major affect-driven processes
having an impact on the collaboration structure. This theory is informed by
research in psychology and artificial intelligence which is reviewed in Chapter
\ref{ch:background}. Our contribution, generally speaking, is to synthesize
prior work on appraisal and collaboration, and motivation to provide a new
theory which describes some of the prominent emotion-regulated goal-driven
phenomena in a dyadic collaboration.

\section{Contributions}

Throughout this work we aim to show how a robot can leverage emotion-driven
processes using appraisal algorithms to improve collaboration with humans. As
such, in this thesis work, we introduce a novel framework, called Affective
Motivational Collaboration (AMC) framework, which allows a robotic agent to
collaborate with a human while incoporating the underlying emotion-driven
processes and the expressed emotion of the human collaborator. Such a framework
is built based on computational models of collaboration and appraisal allowing
for task-driven interaction with robots or other agents. The theoretical
foundation, computational models and algorithms as well as the overall
framework, and the end-to-end evaluation of the framework make the following
contributions:

\begin{enumerate}
  \item \textbf{Introducing \textit{Affective Motivational Collaboration Theory}:}
    
  	(Chapter \ref{ch:amct}) As mentioned earlier, since the theoretical
  	foundation of AMC framework is built on the combination of SharedPlans
  	theory of collaboration \cite{grosz:plans-discourse} and cognitive appraisal
  	theory of emotions \cite{marsella:ema-process-model}
  	\cite{scherer:appraisal-processes}, one of the contributions of our work is
  	to introduce theoretical concepts incorporating key notions of both theories
  	in a dyadic collaboration context. Applying cognitive appraisal theory in the
  	collaboration context is novel. Other models of the appraisal theory have not
  	paid attention to the dynamics of the collaboration.
	
  \item \textbf{Developing new computational models and algorithms for
  \textit{Affective Motivational Collaboration Framework}:}
  
	(Chapter \ref{ch:appraisals}) Another contribution of our work is to create
	computational models and algorithms to compute the value of appraisal variables
	in a dyadic collaboration. We use the collaboration structure to compute
	appraisal variables. Reciprocally, we use the evaluative nature of the
	appraisal to make changes to the collaboration structure as required. We have
	also developed a new algorithm for emotion-driven goal management in the
	context of collaboration. Goal management is one of the important functions of
	emotions during collaboration. Existing models and implementations of emotions
	focus only on how emotions regulate and control internal processes and
	sometimes behaviors. This part of our work shows how appraisal components of
	the self and the human collaborator contributes to goal management as an
	emotion function.
  
  \begin{figure*}
    \centering
    \includegraphics[scale=1.17]{figure/collaborative-robot.png}
    \caption{A robotic arm collaborating with a human to achieve a shared goal
    using \textit{Affective Motivational Collaboration Framework}.}
    \label{fig:collaborative-robot}
  \end{figure*}
  
  \item \textbf{Developing a computational framework based on \textit{Affective
  Motivational Collaboration Theory}:}

  (Chapter \ref{ch:framework}) In order to evaluate our computational models and
  algorithms within an interaction with human collaborators, we have developed
  a computational framework based on our theoretical foundations in
  \textit{Affective Motivational Collaboration Theory}. Our computational
  framework implements the key concepts related to \textit{Affective
  Motivational Collaboration Theory} as well as minimal implementation of other
  processes which are required for validation of the model but are not part of
  this thesis' contributions. The emphasis of the model is on the underlying
  cognitive processes of collaboration and appraisal concepts, rather than the
  Perception and the Action mechanisms.

  \item \textbf{Validating \textit{Affective Motivational Collaboration
  Theory:}}

  (Chapters \ref{ch:appraisals} and \ref{ch:awareness}) We have conducted two
  user studies a) to validate our appraisal algorithms before further
  development of our framework, and b) to investigate the overall functionality
  of our framework within an end-to-end system evaluation with human subjects
  and a robot. The second user study was also conducted to evaluate the benefit
  of using our computational framework in human-robot collaboration. In the
  first user study, we crowd sourced our questionnaires to test our hypothesis
  that humans and our algorithms will provide similar answers to questions
  related to different factors within our appraisal algorithms. In the second
  user study, we investigated the importance of emotional awareness in
  human-robot collaboration, and the overall functionality of the AMC framework
  with the participants in our study environment.
\end{enumerate}

\chapter{Background and Related Work}
\label{ch:background}

\section{Computational Collaboration Theories}

\subsection{Shared-Plans Theory}

\subsection{Joint-Intentions Theory}

\subsection{Hybrid Theories}

\subsection{Similarities and Differences}

\subsection{Applications of Collaboration Theories}

\section{Affective Computing}

\subsection{Affect and Emotions}

\subsection{Functions of Emotions}

\subsection{Motivation and Theory of Mind}

\section{Computational Models of Emotions}

\subsection{Appraisal Theory}

\subsection{Other Computational Models}

\subsection{Similarities and Differences}

\subsection{Applications in Autonomous Agents and Robots}

\chapter{Affective Motivational Collaboration Theory}
\label{ch:amct}

\section{Introduction}

\subsection{Scenario}

\subsection{Example of a Collaborative Interaction}

\section{Design and Architecture}

\subsection{Mechanisms}

\subsection{Functions of Emotions}

\subsection{Mental States}

\subsection{Attributes of Mental States}

\chapter{Appraisal Processes in Collaboration Context}
\label{ch:appraisals}

\section{Introduction}
Current computational theories used for human-robot collaboration specify the
structure of collaborative activities, but are weak on the underlying processes
that generate and maintain these structures. We argue that emotions are crucial
to these underlying processes and we have developed a new computational theory,
called Affective Motivational Collaboration Theory, that combines emotion-based
processes, such as appraisal and coping, with collaboration processes, such as
planning, in a single unified framework. This work is implemented as part of a
larger effort to build robots capable of generating and recognizing emotions in
order to be better collaborators. We have investigated the mutual influences of
affective and collaborative processes in a cognitive theory to support
interaction between humans and robots or virtual agents. We build primarily on
the \textit{cognitive appraisal} theory of emotions and the \textit{SharedPlans}
theory of collaboration to investigate the structure, fundamental processes and
functions of emotions in a collaboration. We have developed new algorithms for
appraisal processes as part of a new overall computational model. We have
evaluated our implemented appraisal algorithms by conducting an online user
study.

Although existing collaboration theories explain the important elements of a
collaboration structure, the underlying processes required to dynamically
create, use, and maintain the elements of this structure are largely
unexplained. For instance, a general mechanism has yet to be developed that
allows an agent to effectively integrate the influence of its collaborator's
perceived or anticipated emotions into its own cognitive mechanisms to prevent
shared task failures while maintaining collaborative behavior. Therefore, a
process view of collaboration must include certain key elements. It should
inherently involve social interactions since all collaborations occur between
social agents, and it should essentially constitute a means of modifying the
content of social interaction as the collaboration unfolds. The underlying
processes of emotions possess these two properties, and social functions of
emotions explain some aspects of the underlying processes in collaboration.
This work is implemented as part of a larger effort to build robots capable of
generating and recognizing emotions in order to be better collaborators.

There is also a communicative aspect of emotions. For instance, emotions are
often intended to convey information to others \cite{goffman:self-presentation}.
Emotions are also involved in verbal behaviors. For instance, an utterance can
include both content and relational meaning. An emotion might appear to be
elicited by the content of the utterance, but in fact be an individual's
response to the relational meaning \cite{planalp:communicating-emotion}. The
interpretation of these relational meanings are handled by the appraisal of
events. Appraisal processes give us a way to view emotion as social
\cite{hooft:sharing-emotions}. Meaning is created by an individual's social
experiences in the social world, and individuals communicate these meanings
through utterances. Consequently, the meaning of these utterances and the
emotional communication change the dynamic of social interactions. A successful
and effective emotional communication necessitates ongoing reciprocal
adjustments between interactants that can happen based on interpretation of each
other's behaviors \cite{parkinson:emotion-social-interaction}. This adjustment
procedure requires a baseline and an assessment procedure. While the components
of the collaboration structure, e.g., shared plan, provide the baseline,
emotion-related processes (e.g., appraisal) provide the assessment procedure.

\section{Appraisal and Collaboration}
In this section, we focus on a small part of a larger framework based on our
Affective Motivational Collaboration Theory. We describe the methods which
retrieve information about the collaboration structure, and are used to compute
the values of appraisal variables. Then, we introduce our algorithms implemented
to compute the value of four appraisal variables depicted in Fig.
\ref{fig:appraisal-collaboration}.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/appraisal-collaboration-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Using Collaboration structure in
  Appraisal (mechanisms in our framework).}}
  \label{fig:appraisal-collaboration}
\end{figure}

\section{Collaboration}
The Collaboration mechanism constructs a hierarchy of goals associated with
tasks in the form of a hierarchical task network (see Fig. \ref{fig:cs}), and
also manages and maintains the constraints and other required details of the
collaboration including the inputs and outputs of individual tasks, the
\textit{preconditions} (specifying whether it is appropriate to perform a task),
and the \textit{postconditions} (specifying whether a just-completed task was
successful). Collaboration also keeps track of the focus of attention, which
determines the salient objects, properties and relations at each point, and
shifts the focus of attention during the interaction.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=1\textwidth]{figure/collaborationStructure-small-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Collaboration structure (shared plan).}}
  \label{fig:cs}
\end{figure}

Here, we briefly describe the methods which retrieve information about the
collaboration structure, and are used in our algorithms to compute the values of
appraisal variables. In these methods, $\varepsilon_t$ is the event
corresponding to time \textit{t}, and $g_t$ is a given goal at time \textit{t}.

\begin{itemize}
  \setlength\itemsep{1mm}
  \item \textit{recognizeGoal($\varepsilon_t$)} returns the unique goal to which
  the given event (action, utterance, or emotional expression) directly
  contributes; it is only one goal since the robot can only do one primitive
  action at a time in our collaboration model, i.e, in the goal tree, a given
  primitive action can only directly contribute to one parent goal. The method
  returns \textit{ambiguous} if it does not recognize a goal in the
  plan\footnote{Ambiguity introduces some extra complexities which are beyond
  scope of this paper.}.
  
  \item \textit{getGoalStatus($g_t$)} returns whether $g_t$'s status is
  \textsc{achieved, failed, blocked, inapplicable, pending,} or \textsc{in
  progress}. In our example, ``Check Connector'' is the current (focused) goal
  and it is \textsc{pending}, and the ``Prepare Panels'' and ``Install Solar
  Panels'' are \textsc{in progress}. The focused goal is the goal that the robot
  currently pursues.
  
  \item \textit{getTopLevelGoal($g_t$)} returns $g_t$'s top level goal.

  \item \textit{precondStatus($g_t$)} returns the status of the precondition for
  the given goal whether it is \textsc{satisfied, unsatisfied} or
  \textsc{unknown}. For instance, the precondition for fixing a panel is whether
  the panel is appropriately located on its frame.
  
  \item \textit{isLive($g_t$)} returns \textit{true} if all the predecessors of
  $g_t$ are \textsc{achieved} and all the preconditions are \textsc{satisfied},
  i.e., \textsc{pending} or \textsc{in progress} goals; otherwise returns
  \textit{false}.
  
  \item \textit{isFocusShift($g_t$)} returns \textit{true} if the given
  goal is not the previous focus (top of the stack); otherwise returns
  \textit{false}.
  
  \item \textit{isNecessaryFocusShift($g_t$)} returns \textit{true} if the
  status of the previous focus was \textsc{achieved}; otherwise returns
  \textit{false} \cite{rich:focused-unfocused-users}.
  
  \item \textit{isPath($g_1$, $g_2$)} returns \textit{true} if there is a path
  between $g_1$ and $g_2$ in a plan tree structure; otherwise returns
  \textit{false}.
  
%   \item \textit{doesContribute($g_t$)} returns whether the given goal
%   contributes to another goal in the higher level of the plan hierarchy. For
%   instance, an abstract (nonprimitive) goal of ``Bring Panels'' contributes to
%   the higher level goal of ``Install Solar Panels''.
  
  \item \textit{getContributingGoals($g_t$)} returns $g_t$'s children.
  
  \item \textit{getPredecessors($g_t$)} returns $g_t$'s predecessors.
  
  \item \textit{getInputs($g_t$)} returns all required inputs for $g_t$. For
  example, the goal ``Fix Panels'' requires inputs such as \textit{welding tool}
  and \textit{panel}.
  
  \item \textit{isAvailable($g_t$)} returns whether the given input is
  available. For instance, whether the \textit{welding tool} is available for the
  goal ``Fix Panels''.
  
%   \item \textit{isAchieved($g_t$)} returns whether the given goal is achieved,
%   i.e., whether all the postconditions of the given goal are \textsc{satisfied}.
  
  \item \textit{isFocused($g_t$)} returns whether the focus is on $g_t$.
  
  \item \textit{getResponsible($g_t$)} returns responsible agent(s) for $g_t$.
  In a dyadic collaboration, both of the agents (jointly) can be partly
  responsible for a nonprimitive goal, while each (self or other) is responsible
  for one or more primitive goals. For instance, both the Robot and the
  Astronaut are responsible for the nonprimitive goal of ``Install Solar
  Panels'', whereas it is only the Robot who is responsible for the
  primitive goal of ``Prepare Measurement Tool''.
\end{itemize}

\section{Appraisal Processes}
\label{sec:appraisal}
We consider four appraisal variables to be the most important appraisal
variables in a collaboration context, i.e., \textit{Relevance} (since other
appraisals are only computed for relevant events), \textit{Desirability}
(since it discriminates facilitating and inhibitory events towards the
collaboration progress), \textit{Expectedness} (since it underlies a
collaborative robot's attention), and \textit{Controllability} (since it is
associated with the agent's coping ability)
\cite{shayganfar:relevance-controllability}. There are other appraisal variables
introduced in psychological \cite{scherer:appraisal-processes} and computational
literature \cite{gratch:domain-independent}. We believe most of these variables
can be straightforwardly added to our appraisal mechanism later. All of the
algorithms in this section use mental states of the robot (discussed in Section
\ref{sec:mental-states}) which are formed based on the collaboration structure.
These algorithms use the corresponding recognized goal of the most recent event
at each turn.

\subsection{Relevance}
Relevance is an important appraisal variable since the other appraisal variables
are meaningful only for relevant events. Relevance as an appraisal variable
measures the significance of an event for the self. An event can be evaluated to
be relevant if it has a non-zero utility \cite{marsella:ema-process-model}.
However, the utility of an event is also influenced by the other collaborator's
emotional expressions as the reflection of the other collaborator's mental state
with respect to the status of the collaborative environment. Other appraisal
models only consider the utility of an event based on the self's goal and plan.

Algorithm \ref{alg:relevance} determines the relevance of the given event with
respect to the current mental state. The relevance of the event depends on the
significance of the event with respect to the collaboration status, which is
determined based on the utility of the event as presented in
\cite{gratch:domain-independent,marsella:ema-process-model}. Our algorithm for
computing the relevance of an event during collaboration involves other factors
that other appraisal models do not consider. For instance, the human's
perceived emotion, recurrence of a belief, or occurrence of a belief about an
unrelated goal by the human play important roles by influencing the utility
of an event during collaboration. As a result, evaluating the relevance of
events can cause a collaborative robot to respond effectively which can
positively impact the status of the shared goal, without dedicating all its
resources to every event.

After perceiving an event, the belief about that event represents the event in
the robot's mental state. \textit{recognizeGoal} returns the goal to which the
current event contributes, unless it is \textit{ambiguous}; $g_{t}$ represents
the shared goal at time (turn) $t$ within the shared plan. We compute the
utility ($-1 \leq \mathcal{U} \leq 1$) of the event using the values of the
attributes associated with the existing beliefs, and the attributes of the
motive associated with the recognized goal (see details below). We use three
belief attributes (see Section \ref{sec:mental-states}) to compute the
belief-related part of the utility:

\begin{algorithm}
	\caption{(Relevance)}
	\label{alg:relevance}
	\begin{algorithmic}[1]
		\Function{IsEventRelevant}{Event $\varepsilon_t$}
 			\Statex
			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
 			\Statex
			\State $\mathcal{U} \gets \Call{getEventUtility}{\mathit{g}_{t}}$ 
			\State $\tau_{t} \gets \Call{getEmotionalThreshold}{\mathit{g}_{t}}$
 			\Statex
			\If {$(\tau_{t} \leq |\mathcal{U}|)$}  
 				\State \Return {{\fontsize{7}{8}\selectfont RELEVANT}}
			\Else 
 				\State \Return {{\fontsize{7}{8}\selectfont IRRELEVANT}}
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{itemize}
  \setlength\itemsep{1mm}
  \item \textit{Strength}: The extent to which the preconditions ($\alpha$),
  postconditions ($\beta$), predecessors ($\lambda$), and contributing goals
  ($\mu$) of a goal are known (\textsc{satisfied} or \textsc{unsatisfied}) makes
  beliefs about the goal stronger. An \textsc{unknown} pre and postcondition
  status of a goal and its predecessors and contributing goals forms weaker
  beliefs. For instance, if one knows all predecessors of a pursued goal (e.g.,
  ``Check Panels'') are \textsc{satisfied} (i.e., ``Fix Panels'' and ``Prepare
  Panels''), failure of the pursued goal will elicit one's negative emotion (due
  to the strong beliefs related to the goal); whereas not knowing the status of
  the goal-related factors (e.g., whether the Astronaut could find the tool to
  fix a panel) causes one to form weaker beliefs about the goal.
  \item \textit{Saliency (S)}: Beliefs related to the focused goal are more
  salient than beliefs related to any other goal in the plan; according to
  Fig. \ref{fig:cs}, if one of the collaborators is preparing a solar panel,
  beliefs related to all of the other \textit{live} (\textsc{pending} or
  \textsc{in progress}) goals (e.g. ``Connect Adaptor'') will be less salient
  than beliefs related to the focused goal, i.e., ``Check Connector''. Beliefs'
  saliency decreases according to their corresponding \textit{live} goal's
  distance from the focused goal in the shared plan. \textit{Non-live} goals
  will not be salient.
  \item \textit{Persistence (P)}: The recurrence of a belief over time (turns)
  increases the persistence of the belief. Beliefs occurring only once have the
  lowest value of persistence. For instance, if the Astronaut keeps saying that
  she can not find the measurement tool to check the connector, Robot could  
  pursue a new goal outside of the shared plan to acknowledge Astronaut's
  concern.
\end{itemize}

\noindent We also use two motive attributes discussed in Section
\ref{sec:mental-states} to compute the motive related part of the utility
($\mathcal{U}$):

\begin{itemize}
  \setlength\itemsep{1mm}
  \item \textit{Urgency ($\gamma$)}: There are two factors impacting the urgency
  of a motive: a) whether the goal directing the given motive is the predecessor of
  another goal for which the other collaborator is responsible, and b) whether
  achieving the goal directing the given motive can mitigate the other
  collaborator's negative valenced emotion. For instance, if the Robot has a
  private goal to fetch another panel while the Astronaut is waiting for the
  Robot to connect the adaptor, connecting the adaptor will be more urgent than
  Robot's private goal.
  \item \textit{Importance ($\eta$)}: A motive is important if failure of the
  directing goal causes an impasse in the shared plan (i.e., no further goal is
  available to achieve), or achievement of the directing goal removes an
  existing impasse. For example, if the Robot cannot find the adaptor (an
  impasse to connect the adaptor), and the Astronaut provides another adaptor
  (external motive), the new motive becomes important to remove the impasse in
  the shared plan.
\end{itemize}

We provide the utility function ($\mathcal{U}$) in Equation \ref{eqn:utility}.
This function uses: saliency (\textit{S}) and persistence (\textit{P}) of the
belief related to the recognized goal, the recognized goal's status
($\upsilon$), and the aggregation of belief and motive attributes ($\Psi$)
according to Equation \ref{eqn:power}.

\begin{equation}
    \mathcal{U}(\varepsilon_t)= 
    \begin{dcases}
       \upsilon\!P\cdot S^{\Psi} & \Psi \textgreater 0 \\
       0               			 & \Psi = 0
    \end{dcases}
    \label{eqn:utility}
\end{equation}

Intuitively, we use $\upsilon$ to generate positive and negative utility values.
The $\upsilon$'s value becomes +1 if the status of the corresponding goal is
\textsc{achieved}, \textsc{pending}, or \textsc{in progress}, and $\upsilon$'s
value becomes -1 if the status of the corresponding goal is \textsc{failed,
blocked}, or \textsc{inapplicable}. The \textit{P} influences the value of
utility only as a coefficient since recurrent beliefs are not formed frequently
during collaboration. The $\Psi$ value indicates the magnitude of the influence
of beliefs and motives using their attributes. Hence, the $\Psi$ value impacts
the saliency value of beliefs exponentially, helping to differentiate between
beliefs.

In equation \ref{eqn:power}, the subscript \textit{k} refers to the
\textit{known} goal-related factors (\textsc{satisfied} or
\textsc{unsatisfied}); whereas the subscript \textit{all} includes both
\textit{known} and \textit{unknown} goal-related factors. In this equation, both
urgency ($\gamma$) and importance ($\eta$) attributes of motives can impact the
outcome of the goal-related belief attributes' ratio, and ultimately the $\Psi$
value.

\begin{equation}
    \Psi = \frac{\alpha_{_k} + \beta_{_k} + \lambda_{_k} +
    \mu_{_k}}{\alpha_{_{all}} + \beta_{_{all}} + \lambda_{_{all}} +
    \mu_{_{all}}} + \eta + \gamma
    \label{eqn:power}
\end{equation}

\begin{center} 
    $\eta, \gamma \in \mathbb{N}, \qquad\qquad \eta, \gamma \geq 0$\\
    $\alpha_{_k}, \beta_{_k}, \lambda_{_k}, \mu_{_k} \in \mathbb{N},
    \qquad\qquad \alpha_{_k}, \beta_{_k}, \lambda_{_k}, \mu_{_k} \geq 0$\\
    $\alpha_{_{all}}, \lambda_{_{all}}, \mu_{_{all}} \in \mathbb{N},
    \qquad\qquad \alpha_{_{all}}, \lambda_{_{all}}, \mu_{_{all}} \geq 0$\\
    $\beta_{_{all}} \in \mathbb{N}, \qquad\qquad \beta_{_{all}} \geq 1$
\end{center}

The significance of an event in a collaborative environment is based on the
utility of the event and the human's perceived emotion. The human's perceived
emotion influences the relevance of the event in the form of a threshold value
$\tau_{t}$. In Equation \ref{eqn:threshold}, we use the valence of the perceived
emotion ($\mathcal{V}_{e_h}$) to compute $\tau_{t}$.

\begin{equation}
    \tau_{t}= 
    \begin{dcases}
       1-\mathcal{V}_{e_h} & \mathcal{V}_{e_h} > 0 \\
       |\mathcal{V}_{e_h}| & \mathcal{V}_{e_h} \leq  0
    \end{dcases}
    \label{eqn:threshold}
\end{equation}

\begin{center} 
    $\mathcal{V}_{e_h} \in \mathbb{R}, \qquad\qquad -1 \leq \mathcal{V}_{e_h}
    \leq 1$
\end{center}

Hence, perceiving human's positive emotion (e.g., happiness) reduces the
threshold value which makes the robot find an event \textsc{relevant} with even
a slightly positive utility. Similarly, an event can be considered
\textsc{irrelevant} even though the utility has a relatively positive value,
because of perceiving the human's negative emotion.

\subsection{Desirability}
Desirability characterizes the value of an event to the robot in terms of
whether the event facilitates or thwarts the collaboration goal. Desirability
captures the valence of an event with respect to the robot's preferences
\cite{gratch:domain-independent}. In a collaborative robot, preferences are
biased towards those events facilitating progress in the collaboration.
Desirability plays an important role in the overall architecture; it makes the
processes involved in the other mechanisms (e.g., Motivation and Theory of
Mind) and consequently the robot's mental state, congruent with the
collaboration status which is a collaborative robot's desire. Therefore, it
causes the robot to dismiss events causing inconsistencies in the robot's
collaborative behavior. Moreover, desirability is also crucial from the
collaboration's point of view.

% A collaborative robot needs to know whether its own and the other collaborator's
% actions, utterances, and emotional expressions are desirable in terms of their
% consistence with the status of the current shared goal. In other words, the
% collaboration mechanism uses the appraisal process of desirability to coordinate
% what the self or the other does, says, and expresses during collaboration.
% Reciprocally, the appraisal mechanism and in this case the desirability process
% use the collaboration structure to obtain their required information.

\begin{algorithm}[tbh]
	\caption{(Desirability)}
	\label{alg:desirability}
	\begin{algorithmic}[1]
		\Function{IsEventDesirable}{Event $\varepsilon_t$}
			\Statex
			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
			\State $\mathit{g}_{top} \gets \textit{getTopLevelGoal}{(\mathit{g}_{t})}$
			\Statex
			\If {(\textit{getGoalStatus($g_{top}$)} =
			{\fontsize{8}{9}\selectfont ACHIEVED})}
			\State \Return {{\fontsize{8}{9}\selectfont MOST-DESIRABLE}} 
			\ElsIf {(\textit{getGoalStatus($g_{top}$)} =
			{\fontsize{8}{9}\selectfont FAILED})} 
			\State \Return {{\fontsize{8}{9}\selectfont MOST-UNDESIRABLE}}
			\ElsIf {(\textit{getGoalStatus($g_{top}$)} =
			{\fontsize{8}{9}\selectfont BLOCKED}) \OR\\
			\hspace*{5mm}(\textit{getGoalStatus($g_{top}$)} =
			{\fontsize{8}{9}\selectfont INAPPLICABLE})}
			\State \Return {{\fontsize{8}{9}\selectfont UNDESIRABLE}} 
			\ElsIf {(\textit{getGoalStatus($g_{top}$)} =
			{\fontsize{8}{9}\selectfont PENDING}) \OR\\
			\hspace*{5mm}(\textit{getGoalStatus($g_{top}$)} =
			{\fontsize{8}{9}\selectfont INPROGRESS})}
				\Statex
				\If {(\textit{getGoalStatus($g_{t}$)} =
				{\fontsize{8}{9}\selectfont ACHIEVED})}
				\State \Return {{\fontsize{8}{9}\selectfont DESIRABLE}}
				\ElsIf {(\textit{getGoalStatus($g_{t}$)} = {\fontsize{8}{9}\selectfont
				FAILED})} 
				\State \Return {{\fontsize{8}{9}\selectfont MOST-UNDESIRABLE}}
				\ElsIf {(\textit{getGoalStatus($g_{t}$)} = {\fontsize{8}{9}\selectfont
				BLOCKED}) \OR \\
				\hspace*{10mm}(\textit{getGoalStatus($g_{t}$)} =
				{\fontsize{8}{9}\selectfont INAPPLICABLE})} 
				\State \Return {{\fontsize{8}{9}\selectfont UNDESIRABLE}}
				\ElsIf {(\textit{getGoalStatus($g_{t}$)} =
				{\fontsize{8}{9}\selectfont PENDING}) \OR \\ \hspace{1mm} 
				\hspace*{8mm}(\textit{getGoalStatus($g_{t}$)} =
				{\fontsize{8}{9}\selectfont INPROGRESS})} 
				\State \Return {{\fontsize{8}{9}\selectfont NEUTRAL}}
				\EndIf
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:desirability} provides a process in which the desirability of
an event is computed with regard to the status of the shared goal; i.e., it
operates based on whether and how the event changes the status of the current
shared goal. It distinguishes between the top level goal and the current goal
because the top level goal's change of status attains a higher positive or
negative value of desirability. For instance, failure of the top level goal
(e.g., installing solar panel) is more undesirable than failure of a primitive
goal (e.g., measuring the quality of the installed panel).

% An \textsc{ambiguous} goal is a goal associated with the current event
% ($\varepsilon_t$) which is not recognized in the robot's plan; therefore it is
% \textsc{undesirable} for a collaborative robot. 

A top level goal' status must be \textsc{achieved} (i.e., \textsc{satisfied}
postcondition) to consider the event \textsc{most-desirable}. When the goal's
status is \textsc{failed} (i.e., \textsc{unsatisfied} postcondition) or
\textsc{blocked}, the associated event has the \textsc{most-undesirable} or
\textsc{undesirable} values respectively. A goal is \textsc{blocked} if any of
the required goals or goals recursively through the parent goal are not
\textsc{achieved}. An \textsc{inapplicable} goal is also considered as
\textsc{undesirable}. A goal is \textsc{inapplicable} if any of its predecessors
are not \textsc{achieved}, and/or its preconditions are not \textsc{satisfied}.
For \textsc{pending} and \textsc{inprogress} top level goals, the status of the
current goal associated with the top level goal determines the status of the
event $\varepsilon_t$. Only a non-primitive goal can have \textsc{inprogress}
status, if it has been started but is not yet completed. A goal can be
\textsc{pending} if it is live, or if it is a non-primitive goal that has not
been started yet. \textsc{Achieved} current goals mark an event
($\varepsilon_t$) as \textsc{desirable}, while \textsc{failed} or
\textsc{blocked} current goals render the event associated with them as
\textsc{most-undesirable} and \textsc{undesirable} respectively.
\textsc{Pending} or \textsc{inprogress} current goals mark their associated
events as \textsc{neutral}.

\subsection{Expectedness}
Expectedness is the extent to which the truth value of a state could have been
predicted from causal interpretation of an event. In the collaboration context
the expectedness of an event evaluates the congruency of the event with respect
to the existing knowledge about the shared goal. Thus, expectedness underlies a
collaborative robot's attention. The collaboration mechanism uses expectedness
to maintain the robot's attention and subsequently its mental state with respect
to the shared goal. Reciprocally, the appraisal mechanism uses the underlying
information of the collaboration structure to evaluate the expectedness of an
event \cite{shayganfar:appraisal-short}.

\begin{algorithm}
	\caption{(Expectedness)}
	\label{alg:expectedness}
	\begin{algorithmic}[1]
		\Function{IsEventExpected}{Event $\varepsilon_t$}
			\Statex
			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
			\State $\mathit{g}_{top} \gets \textit{getTopLevelGoal}{(\mathit{g}_{t})}$
			\Statex
			\If {$(\textit{isLive}{(\mathit{g}_{t})})$}
				\If {$(\neg \textit{isFocusShift}{(\mathit{g}_{t})}\hspace*{2mm}\OR$ \\
				\hspace*{13mm}$\textit{isNeccessaryFocusShift}{(\mathit{g}_{t})})$}
				\State \Return {\fontsize{7}{8}\selectfont MOST-EXPECTED}
				\Else
					\State \Return {\fontsize{7}{8}\selectfont EXPECTED}
				\EndIf
			\Else
				\If {$(\textit{isPath}{(\mathit{g}_{t}, \mathit{g}_{top})})$}
					\State \Return {\fontsize{7}{8}\selectfont UNEXPECTED}
				\Else
					\State \Return {\fontsize{7}{8}\selectfont MOST-UNEXPECTED}
				\EndIf
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg:expectedness} we provide the process of computing the
expectedness based on the shared plan and status of the shared goal. The key
point in this algorithm is the status of the current shared goal
($\mathit{g}_{t}$), which is associated with the event $\varepsilon_t$ and its
relationship with the top level goal ($\mathit{g}_{_{top}}$).

The intuition captured here is that one expects the current goal to be finished
before undertaking another activity, but the goals that can be the next focus of
attention are also to be expected. Therefore, if the goal is live, the algorithm
checks whether the goal has not changed, or whether the interpretation of the
last event results in a necessary focus shift. Shifting the focus to a new goal
is necessary when the former goal is achieved and a new goal is required.
Consequently the new event is the \textsc{most-expected} one. However, even if
the focus shift is not necessary, the new event can be considered as
\textsc{expected}, since the corresponding goal is already live. For goals that
have not yet been started (that is, are not live), the algorithm must determine
how unexpected it would be to pursue one now; if the goal is at least in the
plan, i.e., on the path to the top level goal, it is just \textsc{unexpected}
while any others are \textsc{most-unexpected}.

\subsection{Controllability}
\label{sec:controllability}
Controllability is the extent to which an event can be influenced; it is
associated with a robot's ability to cope with an event
\cite{gratch:domain-independent}. Thus, a robot can determine whether an event's
outcome can be altered by actions under either of the collaborators' control. In
other words, controllability is a measure of a robot's ability to maintain or
change a particular state as a consequence of an event.

\begin{algorithm}
	\caption{(Controllability)}
	\label{alg:controllability}
	\begin{algorithmic}[1]
		\Function{IsEventControllable}{Event $\varepsilon_t$}
 			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
  			\Statex
			\State $\mathcal{M} \gets \Call{GetAgencyRatio}{\mathit{g}_{t}}$ 
			\State $\mathcal{R} \gets \Call{GetAutonomyRatio}{\mathit{g}_{t}}$
 			\Statex
			\State $\mathcal{P} \gets \Call{GetSuccPredecessorsRatio}{\mathit{g}_{t}}$
			\State $\mathcal{I} \gets \Call{GetAvailableInputs}{\mathit{g}_{t}}$
  			\Statex
			\State $\mathcal{V}_{e_h} \gets \Call{getEmotionValence}{\mathit{g}_{t}}$ 
			\State $\omega \gets \Call{getWeights}{\mathit{g}_{t}}$
			\Statex
			\State $\mathcal{X} \gets
			\frac{\omega_{0}\cdot \mathcal{M} + \omega_{1}\cdot \mathcal{R} +
			\omega_{2}\cdot \mathcal{P} + \omega_{3}\cdot \mathcal{I}}{\omega_{0} +
			\omega_{1} + \omega_{2} + \omega_{3}} + \mathcal{V}_{e_h}$
  			\Statex
% 			\State $\tau_{t} \gets \Call{getEmotionalThreshold}{\mathit{g}_{t}}$
 			\Statex
			\If {$(\mathcal{X} > 0)$}
 				\State \Return {{\fontsize{7}{8}\selectfont CONTROLLABLE}}
			\Else 
 				\State \Return {{\fontsize{7}{8}\selectfont UNCONTROLLABLE}}
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Controllability is important for the overall architecture. For instance, the
robot can choose to ask or negotiate about a collaborative task which is not
controllable, or form a new motive to establish an alternative goal for the
current uncontrollable event. In general, other mechanisms in the architecture
use the controllability output in their decision making processes; meanwhile
controllability uses information from the collaboration structure, e.g.,
predecessors of a goal.

An important determinant of one's emotional response is the sense of control
over occurring events. This sense of subjective control is based on one's
reasoning about self's power. For instance, the robustness of one's plan for
executing actions can increase one's sense of power and subsequently the sense
of control. In the collaboration context, we have translated the sense of control
into a combination of four different factors including a) \textit{agency} and b)
\textit{autonomy} of the robot, as well as the ratios of c) \textit{successful
predecessors}, and d) the \textit{available inputs} of a given goal
(i.e., $\mathit{g}_{t}$) in the shared plan.

In Algorithm \ref{alg:controllability}, we partially compute the controllability
of an event based on the above four factors. We use weighted averaging of these
factors to determine their impact on the controllability of an event (line 9).
The value of all these weights are set to \textit{1.0} for the purpose of
simplicity at this stage (\textbf{$\Call{getWeights}{}$}). We will adjust these
weights after further investigating the influence of these factors, and
implementing other mechanisms in the overall architecture. We believe that the
human's perceived emotion also impacts the controllability of an event
(\textbf{$\Call{getEmotionValence}{}$}). The ($-1.0 \leq \mathcal{V}_{e_h} \leq
1.0$) is the valence value of the human's perceived emotion. Positive emotions,
e.g., happiness, possess positive values, and negative emotions, e.g., anger,
have negative values. The magnitude of this value can change with respect to the
intensity of the perceived emotion. Thus, a positive controllability value
indicates that an event is \textsc{controllable}; otherwise
\textsc{uncontrollable}.

% \renewcommand\thealgorithm{4\alph{algorithm}}
% \setcounter{algorithm}{0}
% 
% \begin{algorithm}
% 	\caption{(Get Agency Ratio)}
% 	\label{alg:agency}
% 	\begin{algorithmic}[1]
% 		\Function{GetAgencyRatio}{Event $\varepsilon_t$}
% 			\Statex
% 			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
% 			\Statex
% 			\State $\mathcal{M}_{t} \gets \textit{getActiveMotive}{(\mathit{g}_{t})}$
% 			\Statex
% 			\If {$(\mathcal{M}_{t} \neq \emptyset)$}
% 				\If {$(\mathcal{M}_{t}\cdot type = $
% 				{{\fontsize{8}{8}\selectfont INTERNAL}}$)$} \State \Return {1.0}
% 				\Else
% 					\State \Return {0.0}
% 				\EndIf
% 			\Else
% 				\State \Return {0.0}
% 			\EndIf
% 		\EndFunction 
% 	\end{algorithmic}
% \end{algorithm}

$\Call{\textbf{GetAgencyRatio}}{}$: \textit{Agency} is the capacity of an
individual to act independently in a given environment. In a collaborative
environment collaborators are sometimes required to act independently of each
other. Hence, they need to have some internal motives that are formed based on
their own mental states rather than motives that are reinforced by the other.
These internal motives will lead the collaborators to acquire new intentions
when required. If the robot's mental state possesses only an internal motive
supporting the recognized goal, we consider a maximum agency value denoted as
$\mathcal{M}$ in Algorithm \ref{alg:controllability} (i.e., $\mathcal{M}=1.0$);
otherwise we consider the minimum agency value (i.e., $\mathcal{M}=0.0$). Note
that the process of forming new internal motives is beyond scope of this paper.

% \renewcommand\thealgorithm{4\alph{algorithm}}
% \setcounter{algorithm}{1}
% 
% \begin{algorithm}
% 	\caption{(Get Autonomy Ratio)}
% 	\label{alg:autonomy}
% 	\begin{algorithmic}[1]
% 		\Function{GetAutonomyRatio}{Event $\varepsilon_t$}
% 			\Statex
% 			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
% 			\Statex
% 			\State $\Phi_{\mathit{g}} \gets
% 			\textit{extractContributingGoals}{(\mathit{g}_{t})}$
% 			\Statex
% 			\ForAll {$\phi_{\mathit{g}}^i \in \Phi_{\mathit{g}}$}
% 				\If {$(\textit{getResponsible}{(\phi_{\mathit{g}}^i)} =$
% 				{\fontsize{8}{8}\selectfont SELF})} 
% 					\State $count_{self} \gets count_{self} + 1$
% 				\EndIf
% 			\EndFor
% 			\Statex
% 			\State \Return 
% 			${count_{self} \mathbin{/} {{\Phi_{\mathit{g}}}.total()}}$
% 		\EndFunction 
% 	\end{algorithmic}
% \end{algorithm}

$\Call{\textbf{GetAutonomyRatio}}{}$: \textit{Autonomy} is the ability to make
decisions without the influence of others, and implies acting on one's own and
being responsible for that. In a collaborative environment, tasks are delegated
to the collaborators based on their capabilities. Therefore, each collaborator
is responsible for the delegated task and the corresponding goal. In Algorithm
\ref{alg:controllability}, $\mathcal{R}$ denotes the value of autonomy with
regard to the goal $\mathit{g}_{t}$. This value $(0.0 \leq \mathcal{R} \leq
1.0)$ is the ratio of the number of goals contributing to $\mathit{g}_{t}$ for
which the robot is responsible over the total number of contributing goals, if
the goal associated with the current event is a nonprimitive goal. However, if
the associated goal of the current event corresponds to a primitive goal the
value of $\mathcal{M}$ would be 0.0 or 1.0. In general, higher autonomy leads to
a more positive value of controllability.

% \renewcommand\thealgorithm{4\alph{algorithm}}
% \setcounter{algorithm}{2}
% 
% \begin{algorithm}
% 	\caption{(Get Successful Predecessors Ratio)}
% 	\label{alg:predecessors}
% 	\begin{algorithmic}[1]
% 		\Function{GetSucPredecessorsRatio}{Event $\varepsilon_t$}
% 			\Statex
% 			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
% 			\Statex
% 			\State $\Theta{\mathit{g}} \gets
% 			\textit{extractPredecessors}{(\mathit{g}_{t})}$
% 			\Statex
% 			\ForAll {$\theta_{\mathit{g}}^i \in \Theta_{\mathit{g}}$}
% 				\If {$(\textit{isAchieved}{(\theta_{\mathit{g}}^i)})$}
% 					\State $count_{achieved} \gets count_{achieved} + 1$
% 				\EndIf
% 			\EndFor
% 			\Statex
% 			\State \Return
% 			${count_{achieved} \mathbin{/} {\Theta{\mathit{g}}.total()}}$
% 		\EndFunction 
% 	\end{algorithmic}
% \end{algorithm}

$\Call{\textbf{GetSuccPredecessorsRatio}}{}$: The structure of a shared plan
contains the order of the required \textit{predecessors} of a goal. Predecessors
of a goal, $g_t$, are goals that the collaborators should achieve before trying
to achieve goal $g_t$. We use the ratio of successfully achieved predecessors of
the recognized goal over the total number of predecessors of the same goal. If
all of the predecessors of the given goal are achieved, then $\mathcal{P}=1.0$
which is the maximum value for $\mathcal{P}$. On the contrary, failure of all of
the predecessors will lead to $\mathcal{P}=0.0$. Therefore, a higher
$\mathcal{P}$ value positively impacts the value of controllability for the
current event.

% \renewcommand\thealgorithm{4\alph{algorithm}}
% \setcounter{algorithm}{3}
% 
% \begin{algorithm}
% 	\caption{(Get Available Input Ratio)}
% 	\label{alg:inputs}
% 	\begin{algorithmic}[1]
% 		\Function{GetAvailableInputRatio}{Event $\varepsilon_t$}
% 			\Statex
% 			\State $\mathit{g}_{t} \gets \textit{recognizeGoal}{(\varepsilon_t)}$
% 			\Statex
% 			\State $\mathcal{X}_{\mathit{g}} \gets
% 			\textit{extractInputs}{(\mathit{g}_{t})}$
% 			\Statex
% 			\ForAll {$\chi_{\mathit{g}}^i \in \mathcal{X}_{\mathit{g}}$}
% 				\If {$(\textit{IsAvailable}{(\chi_{\mathit{g}}^i)})$}
% 					\State $count_{available} \gets count_{available} + 1$
% 				\EndIf
% 			\EndFor
% 			\Statex
% 			\State \Return
% 			${count_{available} \mathbin{/} \mathcal{X}_{\mathit{g}}.total()}$
% 		\EndFunction 
% 	\end{algorithmic}
% \end{algorithm}

$\Call{\textbf{GetAvailableInputs}}{}$: Finally, \textit{inputs} of a task are
the required elements that the collaborators use to achieve the specified goal
of the task. These inputs are also part of the structure of a shared plan. We
compute the ratio of the available required inputs over the total required
inputs of the goal associated with the current event. This value (denoted as
$\mathcal{I}$ in Algorithm \ref{alg:controllability}) will be bound between 0.0
and 1.0. Similar to the other factors in the controllability process, the closer
the value of $\mathcal{I}$ gets to 1.0, the more positive impact it has on the
overall controllability value of the event.\\

In summary, the output of these four appraisal processes serves as critical
input for the other mechanisms of the Affective Motivational Collaboration
Framework, shown in Fig. \ref{fig:cpm}. By providing adequate interpretation
of events in the collaborative environment, the appraisal mechanism enables the
robot to carry out proper collaborative behaviors.

\section{Experimental Scenario}

\subsection{Tasks}

\section{Evaluation}

\subsection{Hypothesis}
We conducted a user study to test our hypothesis that humans and our algorithms
will provide similar answers to questions related to different factors used
to compute four appraisal variables: relevance, desirability, expectedness, and
controllability.

\begin{figure*}[tbh]
  \centering
  \includegraphics[width=1\textwidth]{figure/taskModel-croped.pdf}
  \caption{Collaboration Task Model for the Evaluation.}
  \label{fig:taskModel}
  \vspace*{-5mm}
\end{figure*}

\subsection{Procedure}
 We conducted a between-subject user study using an online crowdsourcing website
 -- CrowdFlower\footnote{http://www.crowdflower.com}. We had a questionnaire for
 each appraisal variable. There were 12 questions (including 2 test questions)
 in the controllability and expectedness questionnaires, 14 questions (including
 2 test questions) in the desirability questionnaire, and 22 questions
 (including 3 test questions) in the relevance questionnaire.
 
To minimize the background knowledge necessary for our test subjects, we used a
simple domestic example of preparing a peanut butter and jelly sandwich, and a
hard boiled egg sandwich for a hiking trip. We provided textual and graphical
instructions for both questionnaires; Fig. \ref{fig:taskModel} shows the
corresponding task model. The instructions presented a sequence of hypothetical
collaborative tasks to be carried out by the test subject and an imaginary
friend, Mary, in order to accomplish their goal of preparing two sandwiches. We
also provided a simple definition and an example of each appraisal variable. The
collaboration structure and the instructions were the same for both
questionnaires. The questions introduced specific situations related to the
shared plan, which included blocked tasks and failure or achievement of a shared
goal. Each question provided three answers which were counterbalanced in the
questionnaire. We provided an option like C in all questions (see Fig.
\ref{fig:qs1}), because we did not want to force subjects to choose between two
options when they did not have a good reason. There were two questions designed
based on each factor that we use in our algorithms (see Section
\ref{sec:appraisal}). The questions were randomly placed in the questionnaire.
Fig. \ref{fig:qs1} shows an example question from the relevance questionnaire
which was designed to test whether human subjects perceive saliency as a factor
in relevance. The input for our algorithms was the task model depicted in Fig.
\ref{fig:taskModel}.

\subsection{Measurement}
Each question was designed based on different factors that we use in our
algorithms (see Section \ref{sec:appraisal}). Here, we present four example
questions from the expectedness, controllability, desirability, and relevance
questionnaires, and describe how each question relates to a specific factor
within the corresponding algorithm. The input for our algorithms was the task
model depicted in Fig. \ref{fig:taskModel}.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.85\textwidth]{figure/question-sample-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Example Expectedness Question.}}
  \label{fig:qs1}
  \vspace{-5mm}
\end{figure}

Fig. \ref{fig:qs1} shows the example question from the expectedness
questionnaire. In this example, with respect to Algorithm
\ref{alg:expectedness} (line 6), option A is more expected because the task
related to this option provides the next available task in the focus stack (see
the task model in Fig. \ref{fig:taskModel}). Although the task in option B is
part of the existing task model, it is considered as unexpected by our
algorithm, since it is not live in the plan. We provided option C to determine
whether the human subjects will similarly differentiate between these two
options. This question was presented to the human subjects to determine whether
their decision for the expectedness of this event is similar to the output of
the expectedness algorithm. For this question, the human decision was 97\%
similar to the algorithm's output. Average results for the expectedness
questionnaire are presented in Table \ref{tbl:statistics}.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.85\textwidth]{figure/question-sample2-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Example Controllability Question.}}
  \label{fig:qs2}
  \vspace{-5mm}
\end{figure}

Fig. \ref{fig:qs2} shows an example question from the controllability
questionnaire. The algorithm's output is option B, and is determined by
Algorithm \ref{alg:controllability} (line 3), similarly to the expectedness
example above. In this example, option B is more controllable than option A,
because the self over total ratio of the responsibility of the predecessors of
the given task (see \textit{Autonomy} in Section \ref{sec:controllability}) is
higher than the ratio in option A; i.e., self is responsible to spread peanut
butter on one slice of bread and strawberry jam on another slice of bread. In
this question, the humans decision was 90\% in agreement with the algorithm's
output.

Fig. \ref{fig:qs3} shows an example question from the desirability
questionnaire. The output based on the Algorithm \ref{alg:desirability}
(line 14) is option C, since in both option A and option B, the focus goal
has been achieved successfully. Therefore, in this example, both options A and B
are desirable. The humans' decision was 77\% in agreement with the algorithm's
output in this question.

\begin{figure}[tbh]
  \vspace{-4mm}
  \centering
  \includegraphics[width=0.85\textwidth]{figure/question-sample3-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Example Desirability Question.}}
  \label{fig:qs3}
  \vspace{-5mm}
\end{figure}

In the example shown in Fig. \ref{fig:qs4}, with respect to Algorithm
\ref{alg:relevance}, option A is relevant because of Mary's perceived negative
emotion (see Equation \ref{eqn:utility}). Although option B is relevant (since
it achieves the next goal in the shared plan), 83\% of subjects consider it as
less relevant than option A; we believe this is due to the effect of Mary's
perceived negative emotion which also generates a higher utility value in our
relevance algorithm. Another question also tested belief saliency. However, the
options provided only related to the shared plan (i.e., no human emotions in the
options). In this case 87\% of subjects chose the option that accomplished the
next goal in the shared plan. Interestingly, when confronted with a negative
emotion from their collaborator, human subjects deviated from the shared plan
and found their collaborator's emotion more relevant than the original plan. It
is noteworthy that in both the absence and the presence of emotions the human
subjects chose the more salient option with respect to our definition of
saliency, which was not referenced or provided in the questionnaire.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.85\textwidth]{figure/question-sample4-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Example Relevance Question.}}
  \label{fig:qs4}
  \vspace{-5mm}
\end{figure}

Furthermore, as we mentioned earlier, there were two questions related to each
factor in our algorithms. Because each question was asking about a specific
factor, we were able to perform a sensitivity analysis, similar to the saliency
example presented above. We observed similar results for other factors for all
four variables.

\subsection{Participants}
Each subject group originally had 40 subjects. We limited the subject pools to
those with the highest confidence level on the crowdsourcing website in the
United States, Britain, and Australia. Test questions were included to check the
sanity of the answers. We eliminated subjects providing wrong answers to our
sanity questions, and subjects with answering times less than 2 minutes. The
final number of accepted subjects in each group is provided in Table
\ref{tbl:statistics}.

\begin{table}[htbp]
\vspace*{-3mm}
\centering
\caption{Evaluation Results}
\begin{tabular}{|c|c|c|c|c|} \hline
appraisal variables & \# of subjects & mean & stdev & \textit{p}-value\\ \hline 
Relevance &  29 & 0.713 & 0.107 & $<$0.001\\ \hline
Desirability & 35 & 0.778 & 0.150 & $<$0.001\\ \hline 
Expectedness & 33 & 0.785 & 0.120 & $<$0.001\\ \hline 
Controllability & 33 & 0.743 & 0.158 & $<$0.001\\ \hline
\end{tabular}
\label{tbl:statistics}
\vspace*{-3mm}
\end{table}

\section{Results}
\label{sec:results-crowdsourcing}
Average results and standard deviation of the fractions of subjects' answers
agreeing with our algorithms output for both questionnaires are presented in
Table \ref{tbl:statistics}. Each question had 3 answers. Therefore, a random
distribution would result in 33\% agreement with our algorithms' output.
However, the average ratio indicating similarity between human subjects
decisions and the output of our algorithms is significantly higher than 33\%.
The total number of subjects' answers similar to the \textit{relevance}
algorithm (n=29) averaged 71.3\% (s=10.7\%), the \textit{desirability}
algorithm (n=35) averaged 77.8\% (s=15.0\%), the \textit{expectedness}
algorithm (n=33) averaged 78.5\% (s=12.0\%), and the \textit{controllability}
algorithm (n=33) averaged 74.3\% (s=15.8\%). It is worth noting that the human
subjects agreed 100\% on some questions, while on some other questions there
was a much lower level of agreement. Our results indicate that people largely
performed as our hypothesis predicted. The \textit{p}-values obtained based on a
one-tailed z-test (see Table \ref{tbl:statistics}) show the probability of human
subjects' answers being generated from a random set. The very small
\textit{p}-values indicate that the data set is not random; in fact, the high
percentage of similarity confirms our hypothesis and shows that the algorithms
can help us to model appraisal in a collaboration.

\subsection{Controllability}
\label{sec:controllability-crowdsourcing}

\subsection{Desirability}
\label{sec:desirability-crowdsourcing}

\subsection{Expectedness}
\label{sec:expectedness-crowdsourcing}

\subsection{Relevance}
\label{sec:relevance-crowdsourcing}

\section{Discussion}
\label{sec:discussion-crowdsourcing}

\section{Conclusions}
\label{sec:conclusions-crowdsourcing}

\chapter{Computational Framework}
\label{ch:framework}

\section{System Overview}

\section{Components of the Architecture}

\subsection{Mental States}

\subsection{Collaboration}

\subsection{Appraisal}

\subsection{Coping}

\subsection{Motivation}

\subsection{Theory of Mind}

\subsection{Perception}

\subsection{Action}

\chapter{Improving Human-Robot Collaboration Using Emotional-Awareness}
\label{ch:awareness}

\section{Introduction}

As it was mentioned earlier, collaborative robots need to take into account
humans' internal states while making decisions during collaboration. Humans
express emotions to reveal their internal states in social contexts including
collaboration \cite{breazeal:sociable-interactive-robots}. Due to the existence
of such expressions robots' emotional-awareness can improve the quality of
collaboration in terms of humans' perception of performance and preferences.
Hence, collaborative robots need to include affect-driven mechanisms in their
decision making processes to be able to interpret and generate appropriate
responses and behaviors. Our aim in this setup was to study the importance of
emotional awareness and the underlying affect-driven processes in human-robot
collaboration. We examined how emotional-awareness impacts different aspects of
humans' preferences by comparing the results from our participants collaborating
with an emotion-aware and an emotion-ignorant robot.

\section{Collaborative Behaviors and Emotional-Awareness}

\subsection{Goal Postponement}

\subsection{Goal Management}

\subsection{Task Delegation}

\section{Implementation}
The implementation of this user-study included three separated parts. The first
part incorporated the Affective Motivational Collaboration Framework consisting
of all Mental Processes (see left-side of Figure \ref{fig:framework}) as we
described in Chapter \ref{ch:framework}. The second part was implemented to
receive action commands from the framework and forward them to the robot to
control joints and actuators (see right-side of Figure \ref{fig:framework}).
Finally, a wizard was the third part of this setting. The wizard did not do
anything but informing the robot/framework whether the current performed task by
either of the robot or the participant was achieved successfully. The act of the
wizard was completely invisible for the participants, and the wizard had not
impact on robot's decision other than providing tasks' failure or success.

\begin{figure*}[tbh]
  \centering
  \includegraphics[width=\textwidth]{figure/framework-croped.pdf}
  \caption{{\fontsize{9}{9}\selectfont Computational framework based on
  [title suppressed for anonymity] theory (arrows indicate primary influences
  between mechanisms and data flow).}}
  \label{fig:framework}
  \vspace*{-5mm}
\end{figure*}

\subsection{Framework}
\label{sec:theory}
The framework includes all the mechanisms depicted as mental processes in Figure
\ref{fig:framework} along with the mental states. The mental
states shown in Figure \ref{fig:framework} comprise the knowledge base required
for all of the mechanisms in the overall model. The details about these mental
processes and mental states are described in Chapters \ref{ch:amct} and
\ref{ch:framework}. In this user-study, the Collaboration mechanism uses a
hierarchy of goals associated with tasks in a hierarchical task network
structure depicted in Figure \ref{fig:collaboration_structure}.

\begin{figure*}[tbh]
  \centering
  \includegraphics[width=1\textwidth]{figure/collaborationStructure.pdf}
  \caption{{\fontsize{9}{9}\selectfont Collaboration structure used as the
  task model.}}
  \label{fig:collaboration_structure}
  \vspace*{-5mm}
\end{figure*}

\subsection{Robot Controller}
The robot controller is comprised of two major components: 1) ROS-bridge and 2)
joint controller (see Figure \ref{fig:framework}).
ROS-bridge\footnote{http://wiki.ros.org/rosbridge\_suite} provides an API to ROS
functionality for non-ROS programs which enables us to send action commands from
our framework (implemented in JAVA) to the robot's joint controller. The joint
controller receives action commands and translates them into actual joint and
actuator commands and sends them to the robot.

\section{Experimental Scenario}

Our scenario was based on a table top turn-taking game that we designed to
simulate the installation of a solar panel. Participants had to collaborate
one-on-one with our robot to complete all the given tasks required to install
the solar panel. All the tasks consisted of picking up and placing
collaborators' available pegs on predefined spots on the board (see Figure
\ref{fig:game_board}). Each pick-and-place was associated with the robot's or
the participant's task. The robot and the participants had their own unique
primitive tasks that they had to accomplish in their own turn. The final goal of
installing a solar panel required the robot and the participants to accomplish
their own individual tasks. Failure of any task could create an impasse during
the collaboration.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=1\textwidth]{figure/gameBoard.pdf}
  \caption{The layout of the available spots for the human and the robot to
  place their pegs during the collaboration.}
  \label{fig:game_board}
  \vspace*{-3mm}
\end{figure}

\subsection{The Robot}

We conducted our experiment based on a KUKA Youbot (see Figure
\ref{fig:environment}). The robot was stationary on top of a desk and was able
to pick up and place avaiable pegs corresponding to the robot's task. The robot
was operated based on Robot Operating System (ROS -- indigo) and was receiving
commands through the ROS-bridge from our [Title Suppressed For Anonymity]
framework (see Figure \ref{fig:framework}). We used a touch-screen monitor
providing a simple GUI (see Figures \ref{fig:robot-turn-gui} and
\ref{fig:human-turn-gui}) to a) express robot's positive, negative or neutral
emotion through an emoticon, b) display robot's utterances, c) control
turn-taking process of the collaboration, and d) let the participants express
(report) their positive, negative or neutral emotion for each turn. The robot
used MaryTTS an open-source, multilingual Text-to-Speech Synthesis platform to
provide corresponding speech for its utterances in English.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.7\textwidth]{figure/robot-turn-gui.png}
  \caption{The GUI's look during the robot's turn.}
  \label{fig:robot-turn-gui}
  \vspace*{-3mm}
\end{figure}

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.7\textwidth]{figure/human-turn-gui.png}
  \caption{The GUI's look during the human's turn.}
  \label{fig:human-turn-gui}
  \vspace*{-3mm}
\end{figure}

\subsection{Interaction Paradigms}
\label{sec-interaction-paradigms}
At the beginning of each collaboration the robot asked each participant to
achieve the overall shared goal, i.e., ``installing the solar panel''. Then,
before working towards a new goal, the robot informed the participant about the
higher level nonprimitive goal (e.g. Prepare Panel -- see Figure
\ref{fig:taskModel}) of which the primitives were going to be working towards.
The same procedure was used by the robot if there was a decision to switch to
another nonprimitive due to the failure of a task in achieving the current goal.
After achieving a new primitive goal, the robot either informed the human that
it would pursue the next goal, or it informed and passed the turn to the human
to execute the next task with respect to the human's goal. In case of the
human's turn, the robot waited for the human to do a task, then the wizard let
the robot know whether the human's goal was achieved or not. Afterwards the
robot made a decision about which goal to pursue and informed the human
accordingly. The same procedure was applied to both conditions.

The robot interacted via a) speech, b) the corresponding utterance on the
screen, c) negative, positive and neutral expression of emotion through an
emoticon on the screen. There were two conditions of the robot: 1)
emotion-aware and 2) emotion ignorant. The robot used neutral expression in the
case of emotion-ignorance. The interaction was controlled autonomously by the
framework we discussed in Section \ref{sec:theory} in both the emotion-ignorant
and the emotion-aware cases. The reasoning about which task should be done and
controlling the robot was entirely autonomous. Only the perception of the task
failure or achievement by the robot or by the participant was done by a wizard
monitoring the collaboration outside of the test area. The interaction was
structured based on the exact same goals in an HTN for both conditions. The
robot was using the same utterances in both conditions. In the emotion-aware
condition the robot used a different behavior in comparison with the
emotion-ignorant condition only if the participant was expressing a negative
emotion in case of a failure; i.e., the robot's utterances were identical in
emotion-aware and emotion-ignorant cases if in the latter the participant
reported (expressed) a positive or a neutral emotion.

Three different behaviors could be generated only in the emotion-aware
condition. These three behaviors were 1) mitigating the human's negative emotion
and postponing its own task to help the human, 2) goal-management to switch to
another goal which has lower cost with respect to the human's negative emotion,
and 3) task delegation to the human to overcome the impasse. In each run, the
human had two pre-coordinated task failures, and the robot had one. If the human
expressed negative emotion after the first human task-failure, the robot
responded by mitigating the human's negative emotion by saying  ``It was not
your fault. I can help you with this task" and helping the human by providing a
peg to fulfill the human's task. If the human expressed negative emotion after
the second human task-failure, the robot informed the human that they could
proceed with another task to save time while simultaneously requesting a new peg
(i.e. help) from the supervisor. If the human expressed negative emotion as a
result of the robot's task failure, the robot requested help from the human (who
had the correct peg). In the event that the human expressed positive or neutral
emotion during these three failures, the robot behaved identically in the
emotion-ignorant and the emotion-aware cases, by asking the supervisor for help. 

\subsection{Environment and Tasks}

The environment was set up in the Human-Robot Interaction lab. and included the
robot, the collaboration board on top of a desk, and the participant standing in
front of the robot on the other side of the board (see Figure
\ref{fig:environment}). One of the experimenters monitored the interactions
using a live stream of a camera in a different room. The experimenter provided
only the required perception, i.e., decision on success or failure of the tasks
for the robot, through the entire time of the collaboration (see Section
\ref{sec-interaction-paradigms}).

The tasks were defined based on the HTN structure shown in Figure
\ref{fig:taskModel} and were executed in a turn-taking fashion by either of the
collaborators. For each task either the robot or the participant was responsible
to pick up one of the corresponding pegs from their own inventory and place it
on the right spot which was colored and tagged same as the associated peg. Some
pegs and corresponding spots on the board had hidden magnets which prevented the
pegs from standing upright. Any peg that fell over was considered a failed task. 

\section{Evaluation}
\subsection{Hypothesis}

The non/social functions of emotions impact a collaboration process. Human
collaborators prefer to collaborate with others whose behaviors are influenced
by these functions of emotions depending on the context. We developed seven
hypotheses on positive influence of emotion-awareness and usefulness of emotion
function during collaboration:

\textit{\textbf{Hypothesis 1.}} Subjects will feel closer (likability) to the
emotion-aware robot rather than the emotion-ignorant robot.

\textit{\textbf{Hypothesis 2.}} Subjects will find the emotion-aware robot to be
more trustworthy than the emotion-ignorant robot.

\textit{\textbf{Hypothesis 3.}} Subjects will find the emotion-aware robot to
have better performance in collaboration than the emotion-ignorant robot.

\textit{\textbf{Hypothesis 4.}} Subjects will find the emotion-aware robot to be
more understanding of their feelings than the emotion-ignorant robot.

\textit{\textbf{Hypothesis 5.}} Subjects will find the emotion-aware robot to be
more understanding of their goals than the emotion-ignorant robot.

\textit{\textbf{Hypothesis 6.}} Subjects will feel more satisfied about the
collaboration when working with the emotion-aware robot rather than
emotion-ignorant robot.

\textit{\textbf{Hypothesis 7.}} Subjects will perceive higher level of mutual
satisfaction with emotion-aware robot than emotion-ignorant robot.

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{figure/environment.png}
  \caption{{\fontsize{9}{9}\selectfont Experimental setup.}}
  \label{fig:environment}
  \vspace*{-5mm}
\end{figure}

\vspace*{3mm}
\subsection{Procedure}
\label{sec:procedure}
Participants were first given a brief description of the purpose of the
experiment. After the short introduction, they were asked to review and sign a
consent form. Participants were then provided with a written instruction of
their task and the rules for collaborating with the robot. Then, one of the
experimenters lead them into the experiment room and asked the participants
to asked to answer pre-experiment questionnaires. Afterwards, the experimenter
went through all the details of the instructions with the participants
standing in front of the collaboration board and the robot. The experimenter
confirmed participants' correct understanding of the tasks and informed them
of types of task failures that might occur during the collaboration.
Participants were told that researchers were developing a collaborative robot
and would like their help in evaluating their design. Participants were provided
with identical instructions and randomly assigned to the conditions in the
experiment. They were told that, after their collaboration with the robot, they
would be asked to answer a questionnaire on their experience. After completing
the first round of collaboration, participants answered a post-experiment
questionnaire that measured their perceptions of the robot, the task, and
the collaboration procedure. After answering the first post-experiment
questionnaire, participants were told that they were going to collaborate with
the robot one more time and the robot might not necessarily have the same
collaborative behavior. After completing the second round of collaboration,
participants were asked to answer the second post-experiment questionnaire which
consisted of the same questions as the first post-experiment questionnaire.
After all, participants were asked to answer an open-ended questionnaire which
measured their perception of difference between two runs, their preference of
collaborative robot between two runs, and their reasons of preference.

\subsection{Measurements}
\label{sec:Measurements}
In our study two basic conditions of the robot were tested: a) the
emotion-ignorant condition, b) the emotion-aware condition. We measured
participants' recall of the collaborative behaviors presented by the robot using
an open-ended post-experiment questionnaire. We also specifically asked the
participants what behavior of the robot they liked during their collaboration.
We also evaluated participants' levels of satisfaction, trust, confusion, goal
achievement, mutual understanding of goals, mutual understanding of feelings,
mutual agreement, and also participants' beliefs about the efficiency of
collaboration and their feeling of robot's collaborative behaviors. Seven-point
Likert scales were used in these questionnaire items.

\vspace*{-5mm}
\subsection{Participants}
\label{sec:Participants}
A total of 37 participants participated in the experiment in 74 trials.
Participants were recruited from Worcester Polytechnic Institute's students and
staffs as well as other civilians recruited from outside of the campus. The ages
of the participants varied between 19 and 74 with an average of 34.2 years
before our screening of 4 subjects based on our sanity check questions. After
this screening the ages of the participants varied between 19 and 54 with an
average of 30.8 years old. Of the 33 participants, 21 were female and 12
were male. Each participant participated in 2 trials. In one trial the robot was
aware of human's emotion and in the second trial the robot was ignoring human's
emotion. The order of these two trials were randomly assigned to each
participant. In general we used emotion-ignorant robot first in 16 experiments,
and emotion-aware robot first in 17 experiments.

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Collaboration.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-collaboration}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Emotions.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-emotions}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Goals.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-goals}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Likability.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-likability}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Satisfaction.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-satisfaction}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Trust.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-trust}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figure/Overall-Likability.png}
\caption{Results of the Likert scale surveys for 31 questions. The p-value for
the difference between the means for each question is <0.001, except for Q????,
which is 0.008.}
\label{fig:overall-likability}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/table1-croped.pdf}
\caption{The 31 Likert scale questions organized according to their groups.}
\label{fig:31questions-table}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=1\textwidth]{figure/table2-croped.pdf}
\caption{Open-ended questionnaire questions and results.
(\fontsize{10pt}{12pt}\selectfont{*Note: Because we are evaluating whether
humans prefer an emotion-aware robot, these results are taken as negative test
results when calculating the p-value using the binomial distribution. Only those
participants who clearly indicated a preference for the emotion-aware robot are
taken as positive test results.)}}
\label{fig:Open-Ended-Table}
\vspace*{-5mm}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Collaboration.png}
\caption{Impact of age on results of Likert scale questions related to
human's feeling about collaboration.}
\label{fig:age-collaboration}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Emotions.png}
\caption{Impact of age on results of Likert scale questions related to
robot's understanding of human's emotinos.}
\label{fig:age-emotions}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Goals.png}
\caption{Impact of age on results of Likert scale questions related to
robot's understanding of goals.}
\label{fig:age-goals}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Likability.png}
\caption{Impact of age on results of Likert scale questions related to
likability.}
\label{fig:age-likability}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Performance.png}
\caption{Impact of age on results of Likert scale questions related to
performance.}
\label{fig:age-performance}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Satisfaction.png}
\caption{Impact of age on results of Likert scale questions related to
satisfaction with collaborative partner.}
\label{fig:age-satisfaction}
\end{figure*}

\begin{figure*}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figure/Age-Trust.png}
\caption{Impact of age on results of Likert scale questions related to
trust.}
\label{fig:age-trust}
\end{figure*}

\section{Results}

As discussed in Section \ref{sec:Measurements}, results of the user study were
gathered through a 31-question Likert-scale survey that was given to each
participant after each run with the robot, and through a 5-question open-ended
summary questionnaire at the end of the experiment.

\subsection{7-Point Likert Scale Survey Results}
As mentioned previously, the 7-point Likert scale survey was administered at
the end of the emotion-ignorant run and at the end of the emotion-aware run for
each participant. The 31 questions are generally categorized to evaluate the
humans' perceptions of the following seven categories, with 3-6 questions per
group: (1) the likability of the robot (2) the level of trust the human feels
in the robot (3) the human's perception of the robot's performance (4) the
human's perception of the robot's understanding of human's emotions (5) the
human's perception of the robot's understanding of human's and collaboration's
goals and objectives (6) the human's feeling about the collaboration and (7)
the human's perception of the human's and robot's mutual satisfaction with each
other as collaborative partners. Of the 31 questions, 2 were chosen from each
group, for a total of 14 questions, for presentation in this paper. The
remaining questions were omitted due to space constraints. The questions
presented are provided in Figure \ref{fig:14Questions-Table}. These questions
are chosen as representative of their respective groups of questions, and their
results do not necessarily represent the highest levels of statistical
significance.

The results were analyzed using a two-tailed paired t-test to analyze the
difference of means between the emotion ignorant and the emotion-aware
condition. Refer to Figure \ref{fig:14Questions} for the results. As mentioned
in Section \ref{sec:Participants}, participants were randomly assigned to complete
either the emotion-ignorant or the emotion-aware run first; analysis of the
results revealed no statistically significant difference or consistent pattern
based on which run the participant completed first.

\subsubsection{Likability of the Robot}
\label{sec:Likability}
Questions 1 and 2 addressed the likability of the robot. As shown in Figure
\ref{fig:14Questions}, participants would like to continue working with the emotion-aware
robot significantly more than the emotion-ignorant robot by an average of about 1.5
points; additionally participants felt more close to the emotion-aware robot
than the emotion-ignorant robot by about 2.1 points, on the 7-point scale. This
supports Hypothesis 1, which stated that humans would prefer to work with the
emotion-aware robot over the emotion-ignorant robot.

\subsubsection{Human Trust in the Robot}
\label{sec:Trust}
Questions 3 and 4 were designed to measure the degree of trust that the human
participants felt in the robot. As shown in Figure \ref{fig:14Questions},
participants trusted the emotion-aware robot an average of 1.5 points more than
the emotion-ignorant robot, both in general and in terms of collaboration
performance. In fact, in Question 4, participants rated their trust in the
emotion-aware robot to perform appropriately during collaboration an average of
5.9 on a 7-point Likert scale, where 7.0 would indicate maximum trust; this
indicates that the participants felt that the emotion-aware robot's
collaborative performance was acceptable. This result supports Hypothesis 2,
that posits that human participants would find the emotion-aware robot to be
more trustworthy than the emotion-ignorant robot.

\subsubsection{Perception of the Robot's Performance} 
\label{sec:Performance}
Question 5 (which is reverse-scored) measures the participant's perception of
repetitiveness in the robot during the collaboration. In both conditions,
participants rated the robot as moderately repetitive, with the emotion-ignorant
robot's average response being about 1.1 points higher than the emotion-aware.
This result correlates with several of the open-ended responses which described
the emotion-aware robot's behaviors as ``cute'' and ``interesting'', refer to
Section \ref{sec:Open-Ended}. The participants also felt that the emotion-aware
robot's decisions during collaboration improved their own performance, with an
average rating of 5.4, while the emotion-ignorant robot only received an average
rating of 3.3, indicating that participants felt it was not able to interact in
such a way as to increase the human's performance; refer to results from
Question 6. These results support Hypothesis 3, which posited that humans will
perceive the emotion-aware robot as being more capable than the emotion-ignorant
robot.

\subsubsection{Robot's Understanding of Human Emotions} 
\label{sec:Emotions}
For Questions 7 and 8, participants ranked the emotion-aware robot's
understanding of emotions 2.2 and 1.8 points higher, respectively, than the
emotion-ignorant robot's understanding of emotions, supporting Hypothesis 4.
This category showed the highest total difference between the emotion-ignorant
and the emotion-aware robot.

\subsubsection{Robot's Understanding of Human and Collaboration Goals}
\label{sec:Goals}
Question 9 was a measure of whether the human perceived that the robot cared
about the human's goal. On average, participants provided an average rating for
the emotion-aware robot that was 1.5 points higher than that for the
emotion-ignorant robot. Question 10 measured the human perception of the
robot's commitment to the collaboration; for this measure, the average
participant score assigned to the emotion-aware robot was 6.2 points out of a
maximum of 7 points, indicating that the participants felt that the
emotion-aware robot was strongly committed to the collaboration. The
emotion-ignorant robot received an average rating of 4.4 points, indicating
only moderate commitment. These results strongly support Hypothesis 5, which
posits that humans will feel that the emotion-aware robot will better understand
their goals than the emotion-ignorant robot.

\subsubsection{Human's Feeling about the Collaboration}
\label{sec:Collaboration}
Questions 11 and 12 were designed to gauge how the human participants felt
about the partnership within the collaboration and the outcome of the
collaboration. In the emotion-aware condition, participants scored Questions 11
and 12 as 6.1 and 6.3, respectively, indicating a strong sense of pursuing
mutually agreed-upon goals and very high satisfaction with the overall
collaboration. It is worth noting that the participants scored Question 11 for
the emotion-ignorant case at 5.3 points, leading to a 0.8-point difference
between the two conditions; this is the smallest gap that occurs between the
results for any question, and indicates that the participants felt that the
goals were still partially mutually agreed-upon in the emotion-ignorant case.
However, the general satisfaction with the outcome of the collaboration was
significantly less in the case of the emotion-ignorant robot, at 4.8 points.
These results support Hypothesis 6 that humans will feel greater a greater sense
of mutual collaboration and understanding about the collaboration with the
emotion-aware robot.

\subsubsection{Human Perception of Mutual Satisfaction with Collaborative
Partner}
\label{sec:MutualSatisfaction}
Questions 13 and 14 were designed to measure the human's perception of the
robot's satisfaction with the human, and the human's satisfaction with the
robot, respectively. The participants provided an average response in the
emotion-aware condition of 5.8 and 5.9 to Questions 13 and 14, respectively,
indicating a high level of mutual satisfaction; both answers were about 1.5
lower, on average, in the emotion-ignorant condition. These results indicate a
higher level of satisfaction working with the robot in the emotion-aware
condition, and strongly support Hypothesis 7, which posited that humans will
feel a greater sense of mutual satisfaction with the emotion-aware robot than
the emotion-ignorant robot.

\subsection{Results from the Open-Ended Questionnaire} 
\label{sec:Open-Ended}
As described in Section \ref{sec:procedure}, each participant answered an open-ended
questionnaire at the end of the study. Figure \ref{fig:Open-Ended-Table}
summarizes the questionnaire and which run users preferred for certain
conditions (i.e. emotion-ignorant or emotion-aware). Note that some users either
chose not to state a preference, or provided ambiguous answers regarding which
run they preferred for certain conditions; these results were removed from the analysis.
As shown in Figure \ref{fig:Open-Ended-Table}, 100\% of users unambiguously
preferred the run with the emotion-aware robot. In general, this preference
stemmed from a feeling of closeness and partnership, as seen in these responses:
``the robot had emotions and responded to my emotions. Also, what it said about
my failing was cute and aimed to make me feel better.'' Another example is ``I
liked feeling needed and accounted for; I felt closer to the robot.'' Finally,
``I saw the changes in its feeling, which motivated me to care more about my
act...I also liked that he asked me to correct its failure, although it could
ask the supervisor.''  

When asked in which of the two runs the robot exhibited
behavior that could be useful in a more complex task, 93.75\% chose the
emotion-aware robot. In general, respondents thought that the emotion-aware
robot was better at problem solving, more adaptable, and more capable of
handling the social complexities that occur in collaboration, as shown in
responses such as ``The robot explained motives...which is important to keep a
team communicating and on the same pace.'' Also, ``When we failed he initially
switched to a new task and then came back to the originally failed task. It kept
me from getting irritated and negative.'' Finally, ``The more complex, the more
necessary it is to understand how humans think and operate...an empathetic robot
can adapt, encourage and help.'' It is worth noting that one respondent
preferred the emotion-ignorant case, saying ``In a more complex task it might be
better for the robot to take control and simply tell me what to do; trying to be
understanding and collaborative wouldn't be as important as doing the task
correctly.''

The only question that did not provide statistically significant support in
favor of the emotion-aware robot related to which case the robot exhibited
behavior that could prevent human error. About 40\% of respondents thought that
the emotion-ignorant robot was more likely to prevent human error; however, all
but one of these cited calling the supervisor as the main method of preventing
human error, in spite of the fact that the instructions indicated that the
robot's need to call the supervisor counted against the collaboration. Of the
60\% who thought that the emotion-aware robot was better at preventing human
error, most cited the robot's ability to console the human as the main behavior
that could prevent human error. Respondents indicated that this enabled them to
move on and feel better about the collaboration, as with this response: ``The
robot switched to a different task and we came back to an error later. This
allowed my mind to move away from being frustrated. I was able to complete a
different task which felt like a win - then come back and finish the error.
Making my mind move away from frustration could definitely prevent more
errors.''

When asked in which of the runs the robot exhibited behavior that could improve
the efficiency of the collaboration, 83.9\% responded with the emotion-aware
case; of these, the vast majority stated that this was because of the robot's
ability to change the order of tasks in the event of a failure, and to ask the
human for help.

Finally, when asked in which run the most interesting behavior occurred, 82.8\%
chose the emotion-aware condition. Of these respondents, 12 individuals stated
that the robot's attempt to console the human by saying ``It was not your
fault'' in response to the human's negative emotion that occurred as a
consequence of the human's failed task was the most interesting behavior, and a
majority mentioned that it actually made them feel more positive. Six
participants referred to the robot's ability to understand and express emotion.
Several participants referred to the robot's ability to communicate, including
the ability to ask questions. Of those who responded with the emotion-ignorant
case, most found the ability to call the supervisor, and mechanical functions,
such as gripping, to be most interesting.

\subsection{Impact of Demographics} 
As mentioned in Section \ref{sec:Participants}, we recorded certain demographic
information from each participant, including age and gender. We also had each
participant complete several personality questionnaires. Although it was not the
primary purpose of the study, we investigated the Likert scale results to
determine if there were any relevant trends based on the demographics and
personalities of the participants. A close study of the results did  not reveal
any identifiable pattern based on gender or personality.

Age did reveal an interesting pattern. We divided the participants into two
groups, below 30 years of age and 30 or above. While question-by-question
comparisons revealed only a few statistically significant differences based on
age, a consistent pattern emerged. For each of the fourteen questions presented,
the younger age group reported higher scores for the emotion-aware robot (except
Question 5, which is a reverse-score question). In the emotion-ignorant case,
the younger group still scores the robot higher than the older group for 9
questions; for the other 5 questions, the older group scores the
emotion-ignorant robot higher. In fact, a pattern emerged in which the score
drop between the emotion-aware and the emotion-ignorant case was more for the
younger group than for the older group; only questions 4, 6 and 12 broke this
pattern.

\vspace*{-2mm}
\section{Discussion}
Based on the results, all participants prefer to work with the emotion-aware
robot. Humans find the emotion-aware robot more likable and more trustworthy, as
indicated in the Likert-scale responses and the open-ended questionnaire
responses. Based on the responses, the emotional interaction with the robot can
help create a sense of closeness and enjoyment that makes humans want to
continue working with the robot.

The results also indicate that the emotion aware robot can better maintain a
collaborative relationship. Both Likert-scale responses, see Sections \ref{sec:Goals}
and \ref{sec:MutualSatisfaction} and Open-Ended Questionnaire responses indicate
this. Humans felt a stronger sense of the robot's commitment to the
collaboration, and greater understanding of their goals and emotions from the
robot. Several open-ended responses also indicated that the robot was able to
successfully motivate people and maintain their commitment to the collaboration,
especially when tasks failed. Additionally, as shown in Section \ref{sec:Performance},
humans rated the emotion-aware case much higher than the emotion-ignorant case
when asked which robot's decisions improved their performance, in essence
acknowledging that their collaborator's (i.e. the robot's) decisions had a
significant impact on their performance. As some of the open-ended responses
indicated, successfully managing emotions within the collaboration can help keep
the collaboration on track, and prevent distractions due to guilt and other
negative emotions.

Finally, the emotion-aware robot developed a stronger sense of  partnership
through greater communication. The participants felt better understood by the
emotion-aware robot, and felt that the goals were more mutually agreed-upon,
refer to Sections \ref{sec:Collaboration} and \ref{sec:Emotions}, respectively. As
evidenced in the following response, the emotion-aware robot was successfully
able to create a sense of partnership through its more open communication style:
``Communication is very important. In the first run (i.e. emotion-aware) the
robot states what tasks he is working on, it is clear and straight-forward. Also
during the first run the robot cares about the human(me)'s feelings and cheers
me up when I failed at the tasks, I think that could also improve efficiency of
collaboration, because it would be more like a team or partnership.''

\section{Conclusions}
The goal of our user-study was to compare different aspects of humans'
preferences during collaboration with a robot. Our results conclusively showed
that humans prefer collaborating with an emotion-aware robot which is capable
of a) expressing appropriate emotions, and b) changing collaborative behaviors
based on the perceived negative emotions of the human. We are interested to
carry out further studies with more capabilities from our framework in order to
study collaboration with more complex tasks and evaluate collaborative
performance based on objective measures such as cost or time to achieve a shared
goal. We believe that emotion-aware robots will outperform emotion-ignorant
robots in collaboration with humans.

\chapter{Conclusion}
\label{ch:conclusion}

\section{Discussion}

\section{Future Work}

\pagebreak

\bibliographystyle{abbrv}
\bibliography{mshayganfar}


\begin{appendices}
\chapter*{Appendix A}
\label{apdx:constraints}
\addcontentsline{toc}{chapter}{A}

\end{appendices}

\end{document}